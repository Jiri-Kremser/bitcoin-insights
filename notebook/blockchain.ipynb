{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blockchain analysis\n",
    "\n",
    "In case you know nothing about the Bitcoin and Blockchain, you can start by watching the following video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "IPython.display.HTML('<iframe width=\"750\" height=\"430\" src=\"https://www.youtube.com/embed/Lx9zgZCMqXE?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup\n",
    "\n",
    "Here we will import the `pyspark` module and set up a `SparkSession`. By default, we'll use a `SparkSession` running locally, with one Spark executor; we're dealing with small data, so it doesn't make sense to run against a cluster, but the `local[1]` can be changed with the ip of the Spark cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local[4]\") \\\n",
    "                    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "To obtain the graph representing the transaction in the Bitcoin network, we need to load set of nodes representing the wallets (fingerprints of the public keys) and the set of edges representing each transaction. For this example we will use two parquet files that were generated from the blockchain data by this [convertor](https://github.com/Jiri-Kremser/bitcoin-insights/tree/master/parquet-converter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_nodes = spark.read.load(\"/tmp/nodes1.parquet\") \\\n",
    "                      .withColumnRenamed(\"_1\", \"id\") \\\n",
    "                      .withColumnRenamed(\"_2\", \"Address\")\n",
    "raw_nodes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, each record in the Address column contains a string `bitcoinaddress_<hash>`, where the hash is the actual address. Let's remove the redundant prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = raw_nodes.withColumn(\"Address\", regexp_replace(\"Address\", \"bitcoinaddress_\", \"\")).cache()\n",
    "nodes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also verify, that these addresses are real on https://blockchain.info/address/. \n",
    "\n",
    "Example:\n",
    " 1. get a random BTC address\n",
    " 1. create the link from the address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_address = nodes.rdd.takeSample(False, 1)[0][1]\n",
    "IPython.display.Markdown('link of the random wallet: https://blockchain.info/address/' + random_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_edges = spark.read.load(\"/tmp/edges1.parquet\") \\\n",
    "                      .withColumnRenamed(\"srcId\", \"src\") \\\n",
    "                      .withColumnRenamed(\"dstId\", \"dst\") \\\n",
    "                      .drop(\"attr\") \\\n",
    "                      .cache()\n",
    "raw_edges.show(5)\n",
    "raw_edges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleansing\n",
    "\n",
    "Remove the self-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = raw_edges.filter(\"src != dst\")\n",
    "edges.show(5)\n",
    "edges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Constructing the graph representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "\n",
    "g = GraphFrame(nodes, edges).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Get the vertex degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vertexDegreesAndIds = g.inDegrees.join(g.outDegrees, \"id\")\n",
    "vertexDegrees = vertexDegreesAndIds.drop(\"id\")\n",
    "vertexDegrees.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Calculate some basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexDegrees.describe() \\\n",
    "             .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "_ = sns.countplot([int(i.inDegree) for i in vertexDegrees.sample(False, 0.003).select(\"inDegree\").collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.countplot([int(i.outDegree) for i in vertexDegrees.sample(False, 0.003).select(\"outDegree\").collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"inDegree\", y=\"outDegree\", data=vertexDegrees.sample(False, 0.01, 42).toPandas());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexDegrees.select(corr(\"inDegree\", \"outDegree\")) \\\n",
    "             .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [i*1.0/20 for i in range(0,20)]\n",
    "quantiles = vertexDegrees.approxQuantile(\"inDegree\", probs, 0.03)\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "p_df = pd.DataFrame({'quantiles': quantiles, 'probabilities': probs})\n",
    "\n",
    "sns.lmplot(x=\"probabilities\", y=\"quantiles\", fit_reg=False, data=p_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(id):\n",
    "    nodes = spark.read.load(\"/tmp/nodes\" + id +\".parquet\").withColumnRenamed(\"_1\", \"id\")\n",
    "    raw_edges = spark.read.load(\"/tmp/edges\" + id +\".parquet\") \\\n",
    "                          .withColumnRenamed(\"srcId\", \"src\").withColumnRenamed(\"dstId\", \"dst\") \\\n",
    "                          .drop(\"attr\")\n",
    "\n",
    "    edges = raw_edges.filter(\"src != dst\")\n",
    "    self_loops = raw_edges.count() - edges.count()\n",
    "    print(\"Graph \" + id + \" - self-loops count: \" + str(self_loops))\n",
    "    g = GraphFrame(nodes, edges)\n",
    "    degrees = g.inDegrees.join(g.outDegrees, \"id\").drop(\"id\")\n",
    "    return [g, degrees]\n",
    "\n",
    "# g1 ~ blkfiles 051-054 ~ 2013~04\n",
    "# g2 ~ blkfiles 401-404 ~ 2015~12\n",
    "g1, degrees1 = create_graph(\"1\")\n",
    "g2, degrees2 = create_graph(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees1.describe().show()\n",
    "degrees2.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quantiles(degrees):\n",
    "    probs = [i*1.0/15 for i in range(0,15)]\n",
    "    quantiles = degrees.approxQuantile(\"outDegree\", probs, 0.03)\n",
    "    p_df = pd.DataFrame({'quantiles': quantiles, 'probabilities': probs})\n",
    "    sns.lmplot(x=\"probabilities\", y=\"quantiles\", fit_reg=False, data=p_df)\n",
    "\n",
    "plot_quantiles(degrees1)\n",
    "plot_quantiles(degrees2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Find some patterns in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_with_degrees = GraphFrame(g.vertices.join(vertexDegreesAndIds, \"id\"), edges)\n",
    "graph_with_degrees.vertices.show()\n",
    "\n",
    "\n",
    "motifs = graph_with_degrees.find(\"(a)-[]->(b)\") \\\n",
    "                           .filter(\"a.outDegree > 1000\") \\\n",
    "                           .filter(\"a.inDegree = 1\") \\\n",
    "                           .filter(\"b.outDegree = 1\") \\\n",
    "                           .filter(\"b.inDegree > 1000\")\n",
    "motifs.show()\n",
    "motifs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualization of a sub-graph\n",
    "\n",
    "Our data contain a lot of nodes and edges so let's show only a small fraction of the transaction graph. We will show all the outgoing transaction of particular bitcoin address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "\n",
    "vertexOutDegrees = g.outDegrees\n",
    "senders = vertexOutDegrees.join(nodes, vertexOutDegrees.id == nodes.id) \\\n",
    "                          .drop(\"id\") \\\n",
    "                          .orderBy(\"outDegree\", ascending=False)\n",
    "\n",
    "# feel free to use any address that is present in the dataset\n",
    "address = senders.take(1000)[999].Address\n",
    "\n",
    "sub_graph = g.find(\"(src)-[e]->(dst)\") \\\n",
    "             .filter(col('src.Address') == address)\n",
    "    \n",
    "def node_to_dict(r):\n",
    "    return {\n",
    "        'id': r[0],\n",
    "        'label': r[1],\n",
    "        'x': random.uniform(0,1),\n",
    "        'y': random.uniform(0,1),\n",
    "        'size': random.uniform(0.2,1)\n",
    "    }\n",
    "\n",
    "sub_nodes = sub_graph.select(\"dst.id\", \"dst.Address\").distinct()\n",
    "sub_edges = sub_graph.select(\"e.src\", \"e.dst\")\n",
    "\n",
    "target_nodes_dict = map(node_to_dict, sub_nodes.collect())\n",
    "\n",
    "def edge_to_dict(i, r):\n",
    "    return {\n",
    "        'id': i,\n",
    "        'source': r[0],\n",
    "        'target': r[1]\n",
    "    }\n",
    "\n",
    "sub_edges_dict = [edge_to_dict(i, r) for i, r in enumerate(sub_edges.collect())]\n",
    "\n",
    "target_nodes_dict.append({\n",
    "    'id': sub_edges.first()['src'],\n",
    "    'label': address,\n",
    "    'color': '#999',\n",
    "    'x': -1,\n",
    "    'y': 0.5,\n",
    "    'size': 2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to show the data using the [sigmajs](sigmajs.org) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "    paths: {\n",
    "        sigmajs: 'https://cdnjs.cloudflare.com/ajax/libs/sigma.js/1.2.0/sigma.min'\n",
    "    }\n",
    "});\n",
    "\n",
    "require(['sigmajs']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from string import Template\n",
    "import json\n",
    "\n",
    "js_text_template = Template(open('js/sigma-graph.js','r').read())\n",
    "\n",
    "graph_data = { 'nodes': target_nodes_dict, 'edges': sub_edges_dict }\n",
    "\n",
    "js_text = js_text_template.substitute({'graph_data': json.dumps(graph_data),\n",
    "                                       'container': 'graph-div'})\n",
    "\n",
    "html_template = Template('''\n",
    "<div id=\"graph-div\" style=\"height:400px\"></div>\n",
    "<script> $js_text </script>\n",
    "''')\n",
    "\n",
    "HTML(html_template.substitute({'js_text': js_text}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
