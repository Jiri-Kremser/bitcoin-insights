{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blockchain analysis\n",
    "\n",
    "In case you know nothing about the Bitcoin and Blockchain, you can start by watching the following video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"750\" height=\"430\" src=\"https://www.youtube.com/embed/Lx9zgZCMqXE?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "IPython.display.HTML('<iframe width=\"750\" height=\"430\" src=\"https://www.youtube.com/embed/Lx9zgZCMqXE?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup\n",
    "\n",
    "Here we will import the `pyspark` module and set up a `SparkSession`. By default, we'll use a `SparkSession` running locally, with one Spark executor; we're dealing with small data, so it doesn't make sense to run against a cluster, but the `local[1]` can be changed with the ip of the Spark cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.driver.host', u'10.1.4.234'),\n",
       " (u'spark.ui.reverseProxyUrl', u'/'),\n",
       " (u'spark.app.id', u'app-20170921114512-0001'),\n",
       " (u'spark.master', u'spark://10.1.2.102:7077'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.app.name', u'PySparkShell'),\n",
       " (u'spark.driver.port', u'38073'),\n",
       " (u'spark.jars',\n",
       "  u'file:/opt/spark/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar,file:/opt/spark/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar,file:/opt/spark/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar,file:/opt/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar,file:/opt/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar'),\n",
       " (u'spark.sql.catalogImplementation', u'hive'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.submit.pyFiles',\n",
       "  u'/opt/spark/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar,/opt/spark/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar,/opt/spark/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar,/opt/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar,/opt/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.files',\n",
       "  u'file:/opt/spark/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar,file:/opt/spark/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar,file:/opt/spark/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar,file:/opt/spark/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar,file:/opt/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar'),\n",
       " (u'spark.ui.reverseProxy', u'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#                     .master(\"local[4]\") \\\n",
    "#                     .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#                     .getOrCreate()\n",
    "\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "To obtain the graph representing the transaction in the Bitcoin network, we need to load set of nodes representing the wallets (fingerprints of the public keys) and the set of edges representing each transaction. For this example we will use two parquet files that were generated from the blockchain data by this [convertor](https://github.com/Jiri-Kremser/bitcoin-insights/tree/master/parquet-converter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|              Wallet|\n",
      "+---+--------------------+\n",
      "|  0|bitcoinaddress_93...|\n",
      "|  1|bitcoinaddress_4D...|\n",
      "|  2|bitcoinaddress_BE...|\n",
      "|  3|bitcoinaddress_4B...|\n",
      "|  4|bitcoinaddress_44...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_nodes = spark.read.load(\"/tmp/data/nodes.parquet\") \\\n",
    "                      .withColumnRenamed(\"_1\", \"id\") \\\n",
    "                      .withColumnRenamed(\"_2\", \"Wallet\")\n",
    "raw_nodes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, each record in the wallet column contains a string `bitcoinaddress_<hash>`, where the hash is the actual address of the wallet. Let's remove the redundant prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|              Wallet|\n",
      "+---+--------------------+\n",
      "|  0|9303DBB4C75A56057...|\n",
      "|  1|4D3826A813A4B4E9B...|\n",
      "|  2|BECC6154EEF33464E...|\n",
      "|  3|4B5E0300F11C2932F...|\n",
      "|  4|44730B80C9D5EF65D...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes = raw_nodes.withColumn(\"Wallet\", regexp_replace(\"Wallet\", \"bitcoinaddress_\", \"\")).cache()\n",
    "nodes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, each record in the wallet column contains a string `bitcoinaddress_<hash>`, where the hash is the actual address of the wallet. Let's remove the redundant prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also verify, that these addresses are real on https://blockchain.info/address/. \n",
    "\n",
    "Example:\n",
    " 1. get a random wallet address\n",
    " 1. create the link from the address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "link of the random wallet: https://blockchain.info/address/B4F37D584299E5B1B1C4CF24C463DBBF524DDC2D"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_address = nodes.rdd.takeSample(False, 1)[0][1]\n",
    "IPython.display.Markdown('link of the random wallet: https://blockchain.info/address/' + random_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "|   src|   dst| attr|\n",
      "+------+------+-----+\n",
      "|150102|107378|input|\n",
      "|470403|107378|input|\n",
      "|232249| 97703|input|\n",
      "|539070| 97703|input|\n",
      "|131174|176711|input|\n",
      "+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2087249"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_edges = spark.read.load(\"/tmp/data/edges.parquet\") \\\n",
    "                      .withColumnRenamed(\"srcId\", \"src\") \\\n",
    "                      .withColumnRenamed(\"dstId\", \"dst\") \\\n",
    "                      .cache()\n",
    "raw_edges.show(5)\n",
    "raw_edges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleansing\n",
    "\n",
    "Remove the self-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "|   src|   dst| attr|\n",
      "+------+------+-----+\n",
      "|150102|107378|input|\n",
      "|470403|107378|input|\n",
      "|232249| 97703|input|\n",
      "|539070| 97703|input|\n",
      "|131174|176711|input|\n",
      "+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2081796"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = raw_edges.filter(\"src != dst\")\n",
    "edges.show(5)\n",
    "edges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Constructing the graph representation\n",
    "\n",
    "Spark contains API for graph processing. It's called [graphx](https://spark.apache.org/graphx/) and it also comes with multiple built-in algorithms like page-rank. It uses the [Pregel API](https://spark.apache.org/docs/latest/graphx-programming-guide.html#pregel-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "\n",
    "g = GraphFrame(nodes, edges).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Get the top 10 wallets with respect to the transaction count\n",
    "\n",
    "First, by sorting the nodes by `inDegree` which corresponds to the number of transactions received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(inDegree=121885, Wallet=u'F4B004C3CA2E7F96F9FC5BCA767708967AF67A44'),\n",
       " Row(inDegree=69445, Wallet=u'E1BB16A26D591FD766C1B23FAEC067301AFA8A07'),\n",
       " Row(inDegree=68626, Wallet=u'5605C6DC8A9672A014225BAC565DB25BCEC649A2'),\n",
       " Row(inDegree=5744, Wallet=u'66A731E9FEB460A3A48B5C56BB635B3A409DBD56'),\n",
       " Row(inDegree=3100, Wallet=u'17EBC6064CB035D84DC177CE763D2C93FD5556C6'),\n",
       " Row(inDegree=2338, Wallet=u'A84458A0E8F009B3780CAC779873D298CF22BE8A'),\n",
       " Row(inDegree=2113, Wallet=u'EB9A27CF65B32AABA6FD08D7C4DE65F4C7CF9361'),\n",
       " Row(inDegree=2080, Wallet=u'EFB0CD7206CCF14275643097970DB8A38497B61D'),\n",
       " Row(inDegree=2002, Wallet=u'9B3A7C3A61712270055C1E4AC6FF2704D3ACB1E0'),\n",
       " Row(inDegree=1816, Wallet=u'E1E62CCCDF52C20DA0958A68F1D90A381792D669')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertexInDegrees = g.inDegrees\n",
    "vertexInDegrees.join(nodes, vertexInDegrees.id == nodes.id) \\\n",
    "               .drop(\"id\") \\\n",
    "               .orderBy(\"inDegree\", ascending=False) \\\n",
    "               .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then by using the `outDegree` ~ # transactions sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(outDegree=1994, Wallet=u'7B1DD94268DF8E11BA27AB1C99C61E914C717246'),\n",
       " Row(outDegree=1965, Wallet=u'CBD3148D93C205F862599C080556A2A531146A3B'),\n",
       " Row(outDegree=1952, Wallet=u'9B3A7C3A61712270055C1E4AC6FF2704D3ACB1E0'),\n",
       " Row(outDegree=1694, Wallet=u'580740AC5A1B84C24D7EDB3F3AFC635BDFFB587B'),\n",
       " Row(outDegree=1675, Wallet=u'826AC9812C2BE5FE349A61ACD393F1F38E8C7D2D'),\n",
       " Row(outDegree=1590, Wallet=u'17EBC6064CB035D84DC177CE763D2C93FD5556C6'),\n",
       " Row(outDegree=1577, Wallet=u'EFB0CD7206CCF14275643097970DB8A38497B61D'),\n",
       " Row(outDegree=1568, Wallet=u'A84458A0E8F009B3780CAC779873D298CF22BE8A'),\n",
       " Row(outDegree=1559, Wallet=u'64B7D877B3DB608FD62EC8034770FD0AEFE9975A'),\n",
       " Row(outDegree=1559, Wallet=u'5445DAC9A29FCDE8C7C896CF063923FBED4F5D2C')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertexOutDegrees = g.outDegrees\n",
    "senders = vertexOutDegrees.join(nodes, vertexOutDegrees.id == nodes.id) \\\n",
    "                          .drop(\"id\") \\\n",
    "                          .orderBy(\"outDegree\", ascending=False)\n",
    "senders.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can verify on blockchain.info that the actual number of transaction is lower than what we have just calculated. This stems from the fact how Bitcoin works, when sending BTC from a wallet to another one, it actually sends all the BTC and the receiving node will sends back the rest. We are oversimplyfying here and you can find the details [here](https://en.bitcoin.it/wiki/Transaction) or [here](https://bitcoin.stackexchange.com/questions/9007/why-are-there-two-transaction-outputs-when-sending-to-one-address)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Find circles of length 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# motifs = g.find(\"(a)-[]->(b); (b)-[]->(a)\")\n",
    "# motifs.count()\n",
    "# # motifs.show(5)\n",
    "\n",
    "motifs = g.find(\"(a)-[]->(b)\") \\\n",
    "          .filter(\"a = b\")\n",
    "motifs.count()\n",
    "# motifs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Resource consuming foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o322.run.\n: org.apache.spark.SparkException: Job 17 cancelled because killed via the Web UI\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1439)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1428)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1421)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1683)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat org.apache.spark.graphx.impl.VertexRDDImpl.count(VertexRDDImpl.scala:90)\n\tat org.apache.spark.graphx.Pregel$.apply(Pregel.scala:140)\n\tat org.apache.spark.graphx.lib.LabelPropagation$.run(LabelPropagation.scala:64)\n\tat org.graphframes.lib.LabelPropagation$.org$graphframes$lib$LabelPropagation$$run(LabelPropagation.scala:62)\n\tat org.graphframes.lib.LabelPropagation.run(LabelPropagation.scala:53)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-caf804e9a790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#results.edges.select(\"src\", \"dst\", \"weight\").show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabelPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# this would be nice display(ranks.vertices.orderBy(ranks.vertices.pagerank.desc()).limit(20))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-9f9eb777-883e-4c6b-bfd4-d38671b634d6/userFiles-1853d3e0-59c1-4960-893b-911eca78dce3/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar/graphframes/graphframe.py\u001b[0m in \u001b[0;36mlabelPropagation\u001b[0;34m(self, maxIter)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mvertices\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \"\"\"\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabelPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sqlContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o322.run.\n: org.apache.spark.SparkException: Job 17 cancelled because killed via the Web UI\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1439)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1428)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1421)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1683)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat org.apache.spark.graphx.impl.VertexRDDImpl.count(VertexRDDImpl.scala:90)\n\tat org.apache.spark.graphx.Pregel$.apply(Pregel.scala:140)\n\tat org.apache.spark.graphx.lib.LabelPropagation$.run(LabelPropagation.scala:64)\n\tat org.graphframes.lib.LabelPropagation$.org$graphframes$lib$LabelPropagation$$run(LabelPropagation.scala:62)\n\tat org.graphframes.lib.LabelPropagation.run(LabelPropagation.scala:53)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# this fails with OOM error\n",
    "#results = g.pageRank(resetProbability=0.15, maxIter=1)\n",
    "#results.vertices.select(\"id\", \"pagerank\").show()\n",
    "#results.edges.select(\"src\", \"dst\", \"weight\").show()\n",
    "\n",
    "# g.labelPropagation(maxIter=1)\n",
    "\n",
    "# this would be nice display(ranks.vertices.orderBy(ranks.vertices.pagerank.desc()).limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualization of a sub-graph\n",
    "\n",
    "Our data contain a lot of transactions (2 087 249 transactions among 546 651 wallets) so let's show only a small fraction of the transaction graph. We will show all the outgoing transaction of particular bitcoin address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "\n",
    "# feel free to use any address that is present in the dataset\n",
    "address = senders.take(2000)[1999].Wallet\n",
    "\n",
    "sub_graph = g.find(\"(src)-[e]->(dst)\") \\\n",
    "             .filter(col('src.Wallet') == address)\n",
    "    \n",
    "def node_to_dict(r):\n",
    "    return {\n",
    "        'id': r[0],\n",
    "        'label': r[1],\n",
    "        'x': random.uniform(0,1),\n",
    "        'y': random.uniform(0,1),\n",
    "        'size': random.uniform(0.2,1)\n",
    "    }\n",
    "\n",
    "sub_nodes = sub_graph.select(\"dst.id\", \"dst.Wallet\").distinct()\n",
    "sub_edges = sub_graph.select(\"e.src\", \"e.dst\")\n",
    "\n",
    "target_nodes_dict = map(node_to_dict, sub_nodes.collect())\n",
    "\n",
    "def edge_to_dict(i, r):\n",
    "    return {\n",
    "        'id': i,\n",
    "        'source': r[0],\n",
    "        'target': r[1]\n",
    "    }\n",
    "\n",
    "sub_edges_dict = [edge_to_dict(i, r) for i, r in enumerate(sub_edges.collect())]\n",
    "\n",
    "target_nodes_dict.append({\n",
    "    'id': sub_edges.first()['src'],\n",
    "    'label': address,\n",
    "    'color': '#999',\n",
    "    'x': -1,\n",
    "    'y': 0.5,\n",
    "    'size': 2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to show the data using the [sigmajs](sigmajs.org) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.config({\n",
       "    paths: {\n",
       "        sigmajs: 'https://cdnjs.cloudflare.com/ajax/libs/sigma.js/1.2.0/sigma.min'\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sigmajs']);"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "    paths: {\n",
    "        sigmajs: 'https://cdnjs.cloudflare.com/ajax/libs/sigma.js/1.2.0/sigma.min'\n",
    "    }\n",
    "});\n",
    "\n",
    "require(['sigmajs']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"graph-div\" style=\"height:400px\"></div>\n",
       "<script> var g = {\"nodes\": [{\"y\": 0.8415859899313948, \"x\": 0.7071297023657129, \"size\": 0.2916662010062532, \"id\": 90723, \"label\": \"865BA68EB97FA9623B3B992C425873BCDF64838E\"}, {\"y\": 0.19729894669555237, \"x\": 0.45436836960785043, \"size\": 0.8265437788518644, \"id\": 73489, \"label\": \"52DC8BD47695B6D5413C1C5AC19FFD07D627C00E\"}, {\"y\": 0.7525798848120595, \"x\": 0.41142839591836, \"size\": 0.8845143930031989, \"id\": 531266, \"label\": \"3F621757C623E19278C115D2BA3847BDA9E9B4CC\"}, {\"y\": 0.9775716146985173, \"x\": 0.2975407351449405, \"size\": 0.6859369120623715, \"id\": 36150, \"label\": \"A54EF22D32D5756F9AA4CDB338FF8B13A112734E\"}, {\"y\": 0.47123032970923784, \"x\": 0.11413685263672546, \"size\": 0.8152220945276631, \"id\": 449200, \"label\": \"F4B004C3CA2E7F96F9FC5BCA767708967AF67A44\"}, {\"y\": 0.2174122770958462, \"x\": 0.9621302974788408, \"size\": 0.2970661441672303, \"id\": 402628, \"label\": \"AA45834F735090FEDEB0CA1F166FA78405B28897\"}, {\"y\": 0.2277958562372574, \"x\": 0.008194661364623435, \"size\": 0.7400845914039411, \"id\": 260613, \"label\": \"6FFB8A086E0D91094812F393399D630A5C5973BB\"}, {\"y\": 0.8728757644986687, \"x\": 0.872495089522857, \"size\": 0.48449109477834046, \"id\": 299908, \"label\": \"E3F08554C4D797AB1F817B68ABD12AEC2E9260BB\"}, {\"y\": 0.4395980039933354, \"x\": 0.77628734021799, \"size\": 0.5304081641065781, \"id\": 143918, \"label\": \"A4B1FDB4537D779C79D789DC154544058F94E7B1\"}, {\"y\": 0.24202611189787138, \"x\": 0.9312746926590378, \"size\": 0.4128966481925175, \"id\": 85806, \"label\": \"E59E4A4C94E54B76C9AE10859B8D7AF826F75F8D\"}, {\"y\": 0.3774913477617349, \"x\": 0.2875312448220291, \"size\": 0.25819861296201374, \"id\": 26673, \"label\": \"99C37F14F5CD2AA5B195A7F4D3F8422FA99DA6DC\"}, {\"y\": 0.8220498506994685, \"x\": 0.2808241187438052, \"size\": 0.3327582207544829, \"id\": 98630, \"label\": \"6C8EB8EB53EA37059C20A95708ACBE61B69C76D6\"}, {\"y\": 0.8960150202267259, \"x\": 0.7937477408575561, \"size\": 0.49146167492653936, \"id\": 451006, \"label\": \"28A18D3B87BA226BFA6C04E56149735D2039C904\"}, {\"y\": 0.6066261724162578, \"x\": 0.10330039313984996, \"size\": 0.31410557045845744, \"id\": 244900, \"label\": \"87F97DA212FE06A0B4C2A2609C9474A0BABFBD3E\"}, {\"y\": 0.7993191080576696, \"x\": 0.7354595145111713, \"size\": 0.4882435756441914, \"id\": 228133, \"label\": \"AB4E11D8B737B9F2948F62FDD4D4579F9B270E00\"}, {\"y\": 0.8788501249431505, \"x\": 0.5016547580198154, \"size\": 0.8495351296573312, \"id\": 291988, \"label\": \"4863FCA534DF6069E98842CA19B3F74DD893FC81\"}, {\"y\": 0.7459708763417167, \"x\": 0.3869117674575656, \"size\": 0.6618111705925666, \"id\": 333790, \"label\": \"4A0B307D2D1D419CE5897DAF19B5D8DC59697329\"}, {\"y\": 0.5017022289415575, \"x\": 0.9302558707477133, \"size\": 0.8209586325896399, \"id\": 152448, \"label\": \"0EBFCF01A516F0962B9AC36A66A97CF74BBEAC90\"}, {\"y\": 0.08127448380316693, \"x\": 0.14029901302151637, \"size\": 0.5857762629251486, \"id\": 170350, \"label\": \"0CB310174ECA0BA93A3A0841F4C812FC4B25F1B8\"}, {\"y\": 0.34114750146443085, \"x\": 0.8847307971900336, \"size\": 0.7012258228045583, \"id\": 198854, \"label\": \"A34D9D41DBED29FE3D0F67BB76EB7AD0AFD692EC\"}, {\"y\": 0.7528733085980974, \"x\": 0.7183655601958212, \"size\": 0.6627055230415607, \"id\": 49633, \"label\": \"4DFD4A60C3D5F642971F2AF7A2140755BA7C3418\"}, {\"y\": 0.2271859573772641, \"x\": 0.7076060753914714, \"size\": 0.22816849207574263, \"id\": 36789, \"label\": \"74983C5871243018F5C71AFF2BD1DAEEB40F8BB2\"}, {\"y\": 0.9774817392524133, \"x\": 0.9746712531015547, \"size\": 0.3608791339976916, \"id\": 446648, \"label\": \"564EE8BBACB7658542B2D0AE626081D1A6E88F48\"}, {\"y\": 0.9350417777710929, \"x\": 0.4809545267079641, \"size\": 0.9449250428454825, \"id\": 192506, \"label\": \"C1DA21F74CD0F721DB995D911025198FD39C3C27\"}, {\"y\": 0.29931199248113416, \"x\": 0.5901848857355354, \"size\": 0.4285294080894151, \"id\": 241675, \"label\": \"28216E7DA032140354730C68E6C323AA1AFF650E\"}, {\"y\": 0.048448773390358024, \"x\": 0.8105758470339518, \"size\": 0.28523604370622535, \"id\": 159327, \"label\": \"3F48EE758287715C8720327D85AFC596809F95C1\"}, {\"y\": 0.8466115784525343, \"x\": 0.20198564105896843, \"size\": 0.48384847833937905, \"id\": 324094, \"label\": \"3327A839F540BA291AD0725D1FBD602C3560861A\"}, {\"y\": 0.6856717934629554, \"x\": 0.6520445964622265, \"size\": 0.7068758972560782, \"id\": 24680, \"label\": \"393F6B517681AB5053493D3981ABA6406A465CF7\"}, {\"y\": 0.9450573983235255, \"x\": 0.579794898706569, \"size\": 0.4925500327203078, \"id\": 189874, \"label\": \"1F9DA1BFF29639909ED7B6458EBA92636C09851C\"}, {\"color\": \"#999\", \"label\": \"EC8B7C31883F9E1341EFCD0802B494F5B909740D\", \"y\": 0.5, \"x\": -1, \"id\": 195799, \"size\": 2}], \"edges\": [{\"source\": 195799, \"id\": 0, \"target\": 98630}, {\"source\": 195799, \"id\": 1, \"target\": 98630}, {\"source\": 195799, \"id\": 2, \"target\": 98630}, {\"source\": 195799, \"id\": 3, \"target\": 98630}, {\"source\": 195799, \"id\": 4, \"target\": 299908}, {\"source\": 195799, \"id\": 5, \"target\": 85806}, {\"source\": 195799, \"id\": 6, \"target\": 291988}, {\"source\": 195799, \"id\": 7, \"target\": 291988}, {\"source\": 195799, \"id\": 8, \"target\": 26673}, {\"source\": 195799, \"id\": 9, \"target\": 26673}, {\"source\": 195799, \"id\": 10, \"target\": 228133}, {\"source\": 195799, \"id\": 11, \"target\": 189874}, {\"source\": 195799, \"id\": 12, \"target\": 90723}, {\"source\": 195799, \"id\": 13, \"target\": 90723}, {\"source\": 195799, \"id\": 14, \"target\": 36789}, {\"source\": 195799, \"id\": 15, \"target\": 36789}, {\"source\": 195799, \"id\": 16, \"target\": 36789}, {\"source\": 195799, \"id\": 17, \"target\": 36789}, {\"source\": 195799, \"id\": 18, \"target\": 36789}, {\"source\": 195799, \"id\": 19, \"target\": 531266}, {\"source\": 195799, \"id\": 20, \"target\": 49633}, {\"source\": 195799, \"id\": 21, \"target\": 402628}, {\"source\": 195799, \"id\": 22, \"target\": 402628}, {\"source\": 195799, \"id\": 23, \"target\": 402628}, {\"source\": 195799, \"id\": 24, \"target\": 241675}, {\"source\": 195799, \"id\": 25, \"target\": 152448}, {\"source\": 195799, \"id\": 26, \"target\": 152448}, {\"source\": 195799, \"id\": 27, \"target\": 451006}, {\"source\": 195799, \"id\": 28, \"target\": 192506}, {\"source\": 195799, \"id\": 29, \"target\": 143918}, {\"source\": 195799, \"id\": 30, \"target\": 73489}, {\"source\": 195799, \"id\": 31, \"target\": 449200}, {\"source\": 195799, \"id\": 32, \"target\": 449200}, {\"source\": 195799, \"id\": 33, \"target\": 449200}, {\"source\": 195799, \"id\": 34, \"target\": 449200}, {\"source\": 195799, \"id\": 35, \"target\": 449200}, {\"source\": 195799, \"id\": 36, \"target\": 449200}, {\"source\": 195799, \"id\": 37, \"target\": 449200}, {\"source\": 195799, \"id\": 38, \"target\": 449200}, {\"source\": 195799, \"id\": 39, \"target\": 449200}, {\"source\": 195799, \"id\": 40, \"target\": 449200}, {\"source\": 195799, \"id\": 41, \"target\": 449200}, {\"source\": 195799, \"id\": 42, \"target\": 449200}, {\"source\": 195799, \"id\": 43, \"target\": 449200}, {\"source\": 195799, \"id\": 44, \"target\": 449200}, {\"source\": 195799, \"id\": 45, \"target\": 449200}, {\"source\": 195799, \"id\": 46, \"target\": 449200}, {\"source\": 195799, \"id\": 47, \"target\": 449200}, {\"source\": 195799, \"id\": 48, \"target\": 449200}, {\"source\": 195799, \"id\": 49, \"target\": 449200}, {\"source\": 195799, \"id\": 50, \"target\": 449200}, {\"source\": 195799, \"id\": 51, \"target\": 449200}, {\"source\": 195799, \"id\": 52, \"target\": 449200}, {\"source\": 195799, \"id\": 53, \"target\": 449200}, {\"source\": 195799, \"id\": 54, \"target\": 449200}, {\"source\": 195799, \"id\": 55, \"target\": 449200}, {\"source\": 195799, \"id\": 56, \"target\": 449200}, {\"source\": 195799, \"id\": 57, \"target\": 449200}, {\"source\": 195799, \"id\": 58, \"target\": 449200}, {\"source\": 195799, \"id\": 59, \"target\": 449200}, {\"source\": 195799, \"id\": 60, \"target\": 449200}, {\"source\": 195799, \"id\": 61, \"target\": 449200}, {\"source\": 195799, \"id\": 62, \"target\": 449200}, {\"source\": 195799, \"id\": 63, \"target\": 449200}, {\"source\": 195799, \"id\": 64, \"target\": 449200}, {\"source\": 195799, \"id\": 65, \"target\": 449200}, {\"source\": 195799, \"id\": 66, \"target\": 449200}, {\"source\": 195799, \"id\": 67, \"target\": 449200}, {\"source\": 195799, \"id\": 68, \"target\": 198854}, {\"source\": 195799, \"id\": 69, \"target\": 446648}, {\"source\": 195799, \"id\": 70, \"target\": 333790}, {\"source\": 195799, \"id\": 71, \"target\": 333790}, {\"source\": 195799, \"id\": 72, \"target\": 333790}, {\"source\": 195799, \"id\": 73, \"target\": 24680}, {\"source\": 195799, \"id\": 74, \"target\": 324094}, {\"source\": 195799, \"id\": 75, \"target\": 324094}, {\"source\": 195799, \"id\": 76, \"target\": 170350}, {\"source\": 195799, \"id\": 77, \"target\": 260613}, {\"source\": 195799, \"id\": 78, \"target\": 159327}, {\"source\": 195799, \"id\": 79, \"target\": 244900}, {\"source\": 195799, \"id\": 80, \"target\": 36150}, {\"source\": 195799, \"id\": 81, \"target\": 36150}, {\"source\": 195799, \"id\": 82, \"target\": 36150}]} ;\n",
       "\n",
       "s = new sigma({graph: g, container: 'graph-div', settings: { defaultNodeColor: '#ec5148'} });\n",
       "\n",
       "s.graph.nodes().forEach(function(n) {\n",
       "  n.originalColor = n.color;\n",
       "});\n",
       "s.graph.edges().forEach(function(e) {\n",
       "  e.originalColor = e.color;\n",
       "});\n",
       "\n",
       "s.bind('clickNode', function(e) {\n",
       "  var nodeId = e.data.node.id,\n",
       "      toKeep = neighbors(s.graph, nodeId);\n",
       "  toKeep[nodeId] = e.data.node;\n",
       "\n",
       "  s.graph.nodes().forEach(function(n) {\n",
       "    if (toKeep[n.id])\n",
       "      n.color = n.originalColor;\n",
       "    else\n",
       "      n.color = '#eee';\n",
       "  });\n",
       "\n",
       "  s.graph.edges().forEach(function(e) {\n",
       "    if (toKeep[e.source] && toKeep[e.target])\n",
       "      e.color = e.originalColor;\n",
       "    else\n",
       "      e.color = '#eee';\n",
       "  });\n",
       "\n",
       "  s.refresh();\n",
       "});\n",
       "\n",
       "s.bind('clickStage', function(e) {\n",
       "  s.graph.nodes().forEach(function(n) {\n",
       "    n.color = n.originalColor;\n",
       "  });\n",
       "\n",
       "  s.graph.edges().forEach(function(e) {\n",
       "    e.color = e.originalColor;\n",
       "  });\n",
       "\n",
       "  s.refresh();\n",
       "});\n",
       " </script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from string import Template\n",
    "import json\n",
    "\n",
    "js_text_template = Template(open('js/sigma-graph.js','r').read())\n",
    "\n",
    "graph_data = { 'nodes': target_nodes_dict, 'edges': sub_edges_dict }\n",
    "\n",
    "js_text = js_text_template.substitute({'graph_data': json.dumps(graph_data),\n",
    "                                       'container': 'graph-div'})\n",
    "\n",
    "html_template = Template('''\n",
    "<div id=\"graph-div\" style=\"height:400px\"></div>\n",
    "<script> $js_text </script>\n",
    "''')\n",
    "\n",
    "HTML(html_template.substitute({'js_text': js_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------------+\n",
      "|count|    id|              Wallet|\n",
      "+-----+------+--------------------+\n",
      "|    0| 98630|6C8EB8EB53EA37059...|\n",
      "|    0|299908|E3F08554C4D797AB1...|\n",
      "|    0| 85806|E59E4A4C94E54B76C...|\n",
      "|    0|291988|4863FCA534DF6069E...|\n",
      "|    0| 26673|99C37F14F5CD2AA5B...|\n",
      "|    0|228133|AB4E11D8B737B9F29...|\n",
      "|    0|189874|1F9DA1BFF29639909...|\n",
      "|    0| 90723|865BA68EB97FA9623...|\n",
      "|    0| 36789|74983C5871243018F...|\n",
      "|    0|195799|EC8B7C31883F9E134...|\n",
      "|    0|531266|3F621757C623E1927...|\n",
      "|    0| 49633|4DFD4A60C3D5F6429...|\n",
      "|    0|402628|AA45834F735090FED...|\n",
      "|    0|241675|28216E7DA03214035...|\n",
      "|    0|152448|0EBFCF01A516F0962...|\n",
      "|    0|451006|28A18D3B87BA226BF...|\n",
      "|    0|192506|C1DA21F74CD0F721D...|\n",
      "|    0|143918|A4B1FDB4537D779C7...|\n",
      "|    0| 73489|52DC8BD47695B6D54...|\n",
      "|    0|449200|F4B004C3CA2E7F96F...|\n",
      "+-----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_g = GraphFrame(sub_nodes.union(sub_graph.select(\"src.id\", \"src.Wallet\").distinct()), sub_edges).cache()\n",
    "sub_g\n",
    "\n",
    "# #results = g.pageRank(resetProbability=0.15, maxIter=1)\n",
    "# #results.vertices.select(\"id\", \"pagerank\").show()\n",
    "# #results.edges.select(\"src\", \"dst\", \"weight\").show()\n",
    "# # g.labelPropagation(maxIter)\n",
    "\n",
    "# # this would be nice display(ranks.vertices.orderBy(ranks.vertices.pagerank.desc()).limit(20))\n",
    "\n",
    "\n",
    "labeled = sub_g.triangleCount()\n",
    "labeled.show()\n",
    "\n",
    "# results = sub_g.pageRank(resetProbability=0.15, maxIter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o57.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 19.1 failed 4 times, most recent failure: Lost task 4.3 in stage 19.1 (TID 2023, 10.1.3.190, executor 3): java.io.FileNotFoundException: /tmp/spark-afcf462d-4e08-4cfc-8b5f-19d6bd2e2a30/executor-c44db3ca-9502-4710-b4b9-5de572c135ab/blockmgr-6b74d2d0-9b70-471b-9c92-9233c81c662e/05/shuffle_7_4_0.index.3954f15e-8dda-4e89-a3b4-a81a7249fa48 (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:162)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:144)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:127)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2366)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:245)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: /tmp/spark-afcf462d-4e08-4cfc-8b5f-19d6bd2e2a30/executor-c44db3ca-9502-4710-b4b9-5de572c135ab/blockmgr-6b74d2d0-9b70-471b-9c92-9233c81c662e/05/shuffle_7_4_0.index.3954f15e-8dda-4e89-a3b4-a81a7249fa48 (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:162)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:144)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:127)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7a5e6467a848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtriangles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriangleCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtriangles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o57.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 19.1 failed 4 times, most recent failure: Lost task 4.3 in stage 19.1 (TID 2023, 10.1.3.190, executor 3): java.io.FileNotFoundException: /tmp/spark-afcf462d-4e08-4cfc-8b5f-19d6bd2e2a30/executor-c44db3ca-9502-4710-b4b9-5de572c135ab/blockmgr-6b74d2d0-9b70-471b-9c92-9233c81c662e/05/shuffle_7_4_0.index.3954f15e-8dda-4e89-a3b4-a81a7249fa48 (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:162)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:144)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:127)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2366)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:245)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: /tmp/spark-afcf462d-4e08-4cfc-8b5f-19d6bd2e2a30/executor-c44db3ca-9502-4710-b4b9-5de572c135ab/blockmgr-6b74d2d0-9b70-471b-9c92-9233c81c662e/05/shuffle_7_4_0.index.3954f15e-8dda-4e89-a3b4-a81a7249fa48 (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:162)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:144)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:127)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "triangles = g.triangleCount()\n",
    "triangles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named lib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-4cad225a1019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgraphframes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# f = Graphs(sqlContext).friends()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-9f9eb777-883e-4c6b-bfd4-d38671b634d6/userFiles-1853d3e0-59c1-4960-893b-911eca78dce3/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar/graphframes/examples/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-9f9eb777-883e-4c6b-bfd4-d38671b634d6/userFiles-1853d3e0-59c1-4960-893b-911eca78dce3/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar/graphframes/examples/belief_propagation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named lib"
     ]
    }
   ],
   "source": [
    "from graphframes import examples\n",
    "# f = Graphs(sqlContext).friends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labeled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3c1b1acc5c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabeled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# results.vertices.orderBy(results.vertices.pagerank.asc()).limit(20).show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# results.edges.select(\"src\", \"dst\", \"weight\").show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labeled' is not defined"
     ]
    }
   ],
   "source": [
    "labeled\n",
    "\n",
    "# results.vertices.orderBy(results.vertices.pagerank.asc()).limit(20).show()\n",
    "# results.edges.select(\"src\", \"dst\", \"weight\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled = g.labelPropagation(maxIter=3)\n",
    "g.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
