[![Docker build](https://img.shields.io/docker/automated/jkremser/openshift-spark-with-data.svg)](https://hub.docker.com/r/jkremser/openshift-spark-with-data)
[![Layers info](https://images.microbadger.com/badges/image/jkremser/openshift-spark-with-data.svg)](https://microbadger.com/images/jkremser/openshift-spark-with-data)
## Info

This is the docker image based on `radanalyticsio/openshift-spark:2.2-latest` and
all it does is adding those parquet files to the spark workers. This is a workaround
for the data distribution, because if the notebook is connected to a spark running
in the cluster, the io operations will fail if the data is not present on the workers.

Proper solution would be using persistent volumes and perhaps some distributed file
system, like gluster fs or ceph, but this will make the demos long and boring.

### Building

```bash
make build
```
This will build a docker image with the notebook and the example data. It assumes
the parquet data generated by the converter.
Some example data can be found in `../parquet-converter/data/example1/output/`, but
I suggest creating own from the `~/.bitcoin/blocks/blk00xyz.dat` files using the
`parquet-converter`.

### Running

This image should not be run on localhost. It's purpose is to be deployed via
the Oshinko tool and used as the image for spark master and worker. But it's possible
to run and debug it by invoking `make run`.
